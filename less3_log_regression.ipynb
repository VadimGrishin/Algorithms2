{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 3. Классификация. Логистическая регрессия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhwkeWtb1O0w"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FeKFn2yb1To4"
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "              [1, 1, 2, 5, 3, 0, 5, 10, 1, 2],\n",
    "              [500, 700, 750, 600, 1450, 800, 1500, 2000, 450, 1000],\n",
    "              [1, 1, 2, 1, 2,  1, 3, 3, 1, 2]], dtype = np.float64)\n",
    "y = np.array([0, 0, 1, 0, 1, 0, 1, 0, 1, 1], dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "yXSj4nbxHsFd",
    "outputId": "8d102d54-94bf-4acc-d5b8-d60da152b953"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00e+00, 1.00e+00, 1.00e+00, 1.00e+00, 1.00e+00, 1.00e+00,\n",
       "        1.00e+00, 1.00e+00, 1.00e+00, 1.00e+00],\n",
       "       [1.00e+00, 1.00e+00, 2.00e+00, 5.00e+00, 3.00e+00, 0.00e+00,\n",
       "        5.00e+00, 1.00e+01, 1.00e+00, 2.00e+00],\n",
       "       [5.00e+02, 7.00e+02, 7.50e+02, 6.00e+02, 1.45e+03, 8.00e+02,\n",
       "        1.50e+03, 2.00e+03, 4.50e+02, 1.00e+03],\n",
       "       [1.00e+00, 1.00e+00, 2.00e+00, 1.00e+00, 2.00e+00, 1.00e+00,\n",
       "        3.00e+00, 3.00e+00, 1.00e+00, 2.00e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QU0A16vZHugZ",
    "outputId": "71d74b95-77f4-41a7-e339-c9e7b2004e4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 1., 0., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-aO1NTxOUfo"
   },
   "outputs": [],
   "source": [
    "def calc_std_feat(x):\n",
    "    res = (x - x.mean()) / x.std()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8EL0iGJOVpe"
   },
   "outputs": [],
   "source": [
    "X_st = X.copy()\n",
    "X_st[2, :] = calc_std_feat(X[2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "id": "gviMxz7EOuI3",
    "outputId": "af9a2576-f4d7-41d7-e216-46e0a068cfad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ,  2.        ,  5.        ,  3.        ,\n",
       "         0.        ,  5.        , 10.        ,  1.        ,  2.        ],\n",
       "       [-0.97958969, -0.56713087, -0.46401617, -0.77336028,  0.97958969,\n",
       "        -0.36090146,  1.08270439,  2.11385144, -1.08270439,  0.05155735],\n",
       "       [ 1.        ,  1.        ,  2.        ,  1.        ,  2.        ,\n",
       "         1.        ,  3.        ,  3.        ,  1.        ,  2.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMR5pOA38dDw"
   },
   "outputs": [],
   "source": [
    "def calc_logloss(y, y_pred):\n",
    "    err = - np.mean(y * np.log(y_pred) + (1.0 - y) * np.log(1.0 - y_pred))\n",
    "    err = np.sum(err)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R6zfOHMrBvnX",
    "outputId": "46df0625-963f-4401-da30-b5b42bcf1be7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10536051565782628"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример применения\n",
    "y1 = np.array([1, 0])\n",
    "y_pred1 = np.array([0.9, 0.1])\n",
    "calc_logloss(y1, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEF9rWPNDnss"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    res = 1 / (1 + np.exp(-z))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9tN8lBEEeXU"
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-10, 10, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvIe3RpbEp4l"
   },
   "outputs": [],
   "source": [
    "probabilities = sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "jQsCfht0Et1V",
    "outputId": "0c11fcdd-1cf9-49db-aaa7-4fa520ff840a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, probabilities)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6TH-mkPItb6"
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qtgUN3LW-UIq"
   },
   "outputs": [],
   "source": [
    "def eval_model(X, y, iterations, alpha=1e-4):\n",
    "    np.random.seed(70)\n",
    "    W = np.random.randn(X.shape[0])\n",
    "    n = X.shape[1]\n",
    "    for i in range(1, iterations+2):\n",
    "        z = np.dot(W, X)\n",
    "        \n",
    "        y_pred = sigmoid(z)\n",
    "        err = calc_logloss(y, y_pred)\n",
    "        \n",
    "        grad = alpha * (1/n * np.dot((y_pred - y), X.T))\n",
    "        \n",
    "        if i % (iterations / 10) < 7:\n",
    "            print(i, W, err)\n",
    "            print('---grad:', grad.round(2))\n",
    "            print('---prob', y_pred.round(2))\n",
    "        \n",
    "        W -= grad\n",
    "            \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "oqX7loklBmYZ",
    "outputId": "f4849295-1f14-40d8-c8f2-d1b002e130c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 0.85099741  0.17775492 -0.85303971 -0.44389968] 0.9697836386909907\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.81 0.74 0.67 0.88 0.42 0.67 0.37 0.38 0.82 0.57]\n",
      "2 [ 0.85098419  0.17772386 -0.85301712 -0.44389445] 0.9697668655366443\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.81 0.74 0.67 0.88 0.42 0.67 0.37 0.38 0.82 0.57]\n",
      "3 [ 0.85097098  0.17769281 -0.85299452 -0.44388922] 0.9697500981085272\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.81 0.74 0.67 0.88 0.42 0.67 0.37 0.38 0.82 0.57]\n",
      "4 [ 0.85095776  0.17766177 -0.85297192 -0.44388398] 0.9697333364015289\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.81 0.74 0.67 0.88 0.42 0.67 0.37 0.38 0.82 0.57]\n",
      "5 [ 0.85094455  0.17763074 -0.85294932 -0.44387875] 0.9697165804105454\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.81 0.74 0.67 0.88 0.42 0.67 0.37 0.38 0.82 0.57]\n",
      "6 [ 0.85093134  0.17759972 -0.85292672 -0.4438735 ] 0.9696998301304756\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.81 0.74 0.67 0.88 0.42 0.67 0.37 0.38 0.82 0.57]\n",
      "500 [ 0.84462606  0.16341273 -0.84161795 -0.44080169] 0.9620315506656076\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.87 0.41 0.67 0.36 0.35 0.81 0.56]\n",
      "501 [ 0.84461371  0.16338614 -0.8415948  -0.44079457] 0.9620170898870658\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.87 0.41 0.67 0.36 0.35 0.81 0.56]\n",
      "502 [ 0.84460136  0.16335956 -0.84157165 -0.44078745] 0.9620026328069242\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.87 0.41 0.67 0.36 0.35 0.81 0.56]\n",
      "503 [ 0.84458902  0.16333299 -0.84154849 -0.44078032] 0.9619881794220108\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.87 0.41 0.67 0.36 0.35 0.81 0.56]\n",
      "504 [ 0.84457667  0.16330643 -0.84152534 -0.44077319] 0.961973729729157\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.87 0.41 0.67 0.36 0.35 0.81 0.56]\n",
      "505 [ 0.84456433  0.16327987 -0.84150218 -0.44076606] 0.9619592837251976\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.87 0.41 0.67 0.36 0.35 0.81 0.56]\n",
      "506 [ 0.84455199  0.16325332 -0.84147902 -0.44075892] 0.9619448414069701\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.87 0.41 0.67 0.36 0.35 0.81 0.56]\n",
      "1000 [ 0.83863469  0.15103796 -0.82993639 -0.43685347] 0.9552038998657786\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.86 0.4  0.67 0.35 0.33 0.81 0.56]\n",
      "1001 [ 0.83862305  0.15101492 -0.82991284 -0.43684485] 0.9551909479965888\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.86 0.4  0.67 0.35 0.33 0.81 0.56]\n",
      "1002 [ 0.83861141  0.15099189 -0.82988929 -0.43683623] 0.9551779985608031\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.86 0.4  0.67 0.35 0.33 0.81 0.56]\n",
      "1003 [ 0.83859977  0.15096886 -0.82986574 -0.43682761] 0.9551650515564389\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.86 0.4  0.67 0.35 0.33 0.81 0.56]\n",
      "1004 [ 0.83858814  0.15094584 -0.82984218 -0.43681898] 0.9551521069815152\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.86 0.4  0.67 0.35 0.33 0.81 0.56]\n",
      "1005 [ 0.8385765   0.15092283 -0.82981863 -0.43681035] 0.9551391648340525\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.86 0.4  0.67 0.35 0.33 0.81 0.56]\n",
      "1006 [ 0.83856487  0.15089982 -0.82979508 -0.43680172] 0.9551262251120736\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.8  0.74 0.66 0.86 0.4  0.67 0.35 0.33 0.81 0.56]\n",
      "1500 [ 0.83296377  0.14024976 -0.8180875  -0.43223579] 0.9489947862020867\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.85 0.4  0.67 0.34 0.31 0.81 0.55]\n",
      "1501 [ 0.83295271  0.14022955 -0.81806367 -0.43222598] 0.9489828376187832\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.85 0.4  0.67 0.34 0.31 0.81 0.55]\n",
      "1502 [ 0.83294165  0.14020934 -0.81803985 -0.43221617] 0.9489708906742708\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.85 0.4  0.67 0.34 0.31 0.81 0.55]\n",
      "1503 [ 0.83293059  0.14018914 -0.81801602 -0.43220636] 0.948958945367296\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.85 0.4  0.67 0.34 0.31 0.81 0.55]\n",
      "1504 [ 0.83291953  0.14016894 -0.81799219 -0.43219654] 0.9489470016966072\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.85 0.4  0.67 0.34 0.31 0.81 0.55]\n",
      "1505 [ 0.83290848  0.14014874 -0.81796836 -0.43218673] 0.9489350596609546\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.85 0.4  0.67 0.34 0.31 0.81 0.55]\n",
      "1506 [ 0.83289742  0.14012856 -0.81794453 -0.43217691] 0.9489231192590883\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.85 0.4  0.67 0.34 0.31 0.81 0.55]\n",
      "2000 [ 0.82755607  0.13072886 -0.80612434 -0.42708548] 0.9432016635935883\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.84 0.4  0.67 0.34 0.3  0.8  0.55]\n",
      "2001 [ 0.82754548  0.13071091 -0.80610033 -0.42707472] 0.9431903991496398\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.84 0.4  0.67 0.34 0.3  0.8  0.55]\n",
      "2002 [ 0.8275349   0.13069297 -0.80607632 -0.42706396] 0.9431791358400726\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.84 0.4  0.67 0.34 0.3  0.8  0.55]\n",
      "2003 [ 0.82752432  0.13067503 -0.80605231 -0.4270532 ] 0.9431678736640869\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.84 0.4  0.67 0.34 0.3  0.8  0.55]\n",
      "2004 [ 0.82751374  0.13065709 -0.80602829 -0.42704243] 0.9431566126208845\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.84 0.4  0.67 0.34 0.3  0.8  0.55]\n",
      "2005 [ 0.82750316  0.13063916 -0.80600428 -0.42703167] 0.9431453527096665\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.84 0.4  0.67 0.34 0.3  0.8  0.55]\n",
      "2006 [ 0.82749258  0.13062123 -0.80598027 -0.4270209 ] 0.9431340939296359\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.65 0.84 0.4  0.67 0.34 0.3  0.8  0.55]\n",
      "2500 [ 0.8223663   0.12222625 -0.79408751 -0.42150986] 0.9376960670682845\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.64 0.84 0.39 0.67 0.33 0.29 0.8  0.55]\n",
      "2501 [ 0.82235612  0.12221013 -0.79406339 -0.42149834] 0.9376852832117729\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.64 0.84 0.39 0.67 0.33 0.29 0.8  0.55]\n",
      "2502 [ 0.82234593  0.12219401 -0.79403926 -0.42148682] 0.937674500166192\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.64 0.84 0.39 0.67 0.33 0.29 0.8  0.55]\n",
      "2503 [ 0.82233575  0.12217789 -0.79401514 -0.4214753 ] 0.9376637179310263\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.64 0.84 0.39 0.67 0.33 0.29 0.8  0.55]\n",
      "2504 [ 0.82232556  0.12216178 -0.79399101 -0.42146378] 0.9376529365057611\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.64 0.84 0.39 0.67 0.33 0.29 0.8  0.55]\n",
      "2505 [ 0.82231538  0.12214567 -0.79396688 -0.42145225] 0.9376421558898811\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.64 0.84 0.39 0.67 0.33 0.29 0.8  0.55]\n",
      "2506 [ 0.8223052   0.12212956 -0.79394276 -0.42144073] 0.9376313760828733\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.79 0.73 0.64 0.84 0.39 0.67 0.33 0.29 0.8  0.55]\n",
      "3000 [ 0.81735844  0.11454629 -0.78200832 -0.41559374] 0.9323956862738996\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.83 0.39 0.66 0.33 0.28 0.8  0.54]\n",
      "3001 [ 0.81734859  0.11453164 -0.78198414 -0.41558162] 0.9323852519113134\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.83 0.39 0.66 0.33 0.28 0.8  0.54]\n",
      "3002 [ 0.81733873  0.114517   -0.78195995 -0.41556949] 0.9323748181503358\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.83 0.39 0.66 0.33 0.28 0.8  0.54]\n",
      "3003 [ 0.81732888  0.11450236 -0.78193577 -0.41555736] 0.9323643849906315\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.83 0.39 0.66 0.33 0.28 0.8  0.54]\n",
      "3004 [ 0.81731903  0.11448772 -0.78191159 -0.41554523] 0.932353952431866\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.83 0.39 0.66 0.33 0.28 0.8  0.54]\n",
      "3005 [ 0.81730918  0.11447309 -0.7818874  -0.4155331 ] 0.9323435204737045\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.83 0.39 0.66 0.33 0.28 0.8  0.54]\n",
      "3006 [ 0.81729933  0.11445846 -0.78186322 -0.41552097] 0.9323330891158133\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.83 0.39 0.66 0.33 0.28 0.8  0.54]\n",
      "3500 [ 0.8125036   0.10753417 -0.76991115 -0.4094046 ] 0.9272472930522013\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.28 0.79 0.54]\n",
      "3501 [ 0.81249403  0.10752073 -0.76988695 -0.40939199] 0.9272371230210086\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.28 0.79 0.54]\n",
      "3502 [ 0.81248446  0.10750729 -0.76986275 -0.40937938] 0.927226953454865\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.28 0.79 0.54]\n",
      "3503 [ 0.81247488  0.10749385 -0.76983855 -0.40936676] 0.9272167843535509\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.28 0.79 0.54]\n",
      "3504 [ 0.81246531  0.10748041 -0.76981435 -0.40935414] 0.9272066157168476\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.28 0.79 0.54]\n",
      "3505 [ 0.81245574  0.10746698 -0.76979016 -0.40934153] 0.9271964475445351\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.28 0.79 0.54]\n",
      "3506 [ 0.81244617  0.10745355 -0.76976596 -0.40932891] 0.9271862798363955\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.28 0.79 0.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 [ 0.80777854  0.10106662 -0.75781512 -0.40299646] 0.9222161840835952\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4001 [ 0.80776921  0.10105416 -0.75779094 -0.40298345] 0.9222062226387132\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4002 [ 0.80775988  0.10104171 -0.75776676 -0.40297045] 0.9221962615691612\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4003 [ 0.80775054  0.10102925 -0.75774258 -0.40295744] 0.9221863008747944\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4004 [ 0.80774121  0.1010168  -0.7577184  -0.40294443] 0.9221763405554679\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4005 [ 0.80773188  0.10100435 -0.75769422 -0.40293143] 0.9221663806110376\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4006 [ 0.80772255  0.1009919  -0.75767004 -0.40291842] 0.9221564210413591\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.78 0.72 0.64 0.82 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4500 [ 0.80316445  0.09504492 -0.74573536 -0.39641278] 0.9172795717044895\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.77 0.72 0.63 0.81 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4501 [ 0.80315533  0.09503327 -0.74571123 -0.39639946] 0.9172697821128761\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.77 0.72 0.63 0.81 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4502 [ 0.8031462   0.09502161 -0.74568709 -0.39638614] 0.9172599928372765\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.77 0.72 0.63 0.81 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4503 [ 0.80313707  0.09500997 -0.74566296 -0.39637282] 0.9172502038775951\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.77 0.72 0.63 0.81 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4504 [ 0.80312795  0.09499832 -0.74563882 -0.3963595 ] 0.9172404152337353\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.77 0.72 0.63 0.81 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4505 [ 0.80311882  0.09498667 -0.74561469 -0.39634618] 0.9172306269056018\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.77 0.72 0.63 0.81 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "4506 [ 0.80310969  0.09497503 -0.74559055 -0.39633286] 0.9172208388930988\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.77 0.72 0.63 0.81 0.39 0.66 0.33 0.27 0.79 0.54]\n",
      "5000 [ 0.79864606  0.08938961 -0.73368389 -0.38968871] 0.9124223973177508\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.77 0.71 0.63 0.81 0.39 0.66 0.33 0.26 0.78 0.54]\n",
      "5001 [ 0.79863711  0.08937862 -0.73365982 -0.38967514] 0.9124127552276562\n",
      "---grad: [ 0.  0. -0. -0.]\n",
      "---prob [0.77 0.71 0.63 0.81 0.39 0.66 0.33 0.26 0.78 0.54]\n"
     ]
    }
   ],
   "source": [
    "W = eval_model(X_st, y, iterations=5000, alpha=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jp0AQlnkRBWC"
   },
   "source": [
    "## ДЗ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1*. Измените функцию calc_logloss так, чтобы нули по возможности не попадали в np.log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_logloss1(y, y_pred):\n",
    "\n",
    "    y_pred = np.where(y_pred *(1 - y_pred) == 0, np.abs(y_pred - 10e-9), y_pred)\n",
    "\n",
    "    err = - np.mean(y * np.log(y_pred) + (1.0 - y) * np.log(1.0 - y_pred))\n",
    "    err = np.sum(err)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0000000100247594e-08, 18.420680741439988)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример применения\n",
    "y1 = np.array([1, 0])\n",
    "y_pred1 = np.array([1, 0])\n",
    "y_pred2 = np.array([0, 1])\n",
    "calc_logloss1(y1, y_pred1), calc_logloss1(y1, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Подберите аргументы функции eval_model для логистической регрессии таким образом, чтобы log loss был минимальным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 0.85099741  0.17775492 -0.85303971 -0.44389968] 0.9697836386909907\n",
      "---grad: [ 528.7  1242.48 -903.85 -209.07]\n",
      "---prob [0.81 0.74 0.67 0.88 0.42 0.67 0.37 0.38 0.82 0.57]\n",
      "2 [ -527.84877958 -1242.2999491    902.99869509   208.62926631] nan\n",
      "---grad: [-2000.   -5200.    -226.85 -4000.  ]\n",
      "---prob [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "3 [1472.15122042 3957.7000509  1129.85104421 4208.62926631] nan\n",
      "---grad: [2000.   6800.   -226.85 2800.  ]\n",
      "---prob [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "4 [ -527.84877958 -2842.2999491   1356.70339334  1408.62926631] nan\n",
      "---grad: [-1600.   -5200.    -371.21 -3600.  ]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "5 [1072.15122042 2357.7000509  1727.91632827 5008.62926631] nan\n",
      "---grad: [2000.   6800.   -226.85 2800.  ]\n",
      "---prob [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "6 [ -927.84877958 -4442.2999491   1954.7686774   2208.62926631] nan\n",
      "---grad: [-1600.   -5200.    -371.21 -3600.  ]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "32 [-4527.84877958 -6842.2999491   2676.57160644  7408.62926631] nan\n",
      "---grad: [-1600.   -5200.    -371.21 -3600.  ]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "33 [-2927.84877958 -1642.2999491   3047.78454137 11008.62926631] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "34 [-4527.84877958 -6442.2999491   2965.29277805  8608.62926631] nan\n",
      "---grad: [-1600.   -5200.    -371.21 -3600.  ]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "35 [-2927.84877958 -1242.2999491   3336.50571299 12208.62926631] nan\n",
      "---grad: [2000.   6800.   -226.85 2800.  ]\n",
      "---prob [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "36 [-4927.84877958 -8042.2999491   3563.35806211  9408.62926631] nan\n",
      "---grad: [-1600.   -5200.    -371.21 -3600.  ]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "37 [-3327.84877958 -2842.2999491   3934.57099705 13008.62926631] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "38 [-4927.84877958 -7642.2999491   3852.07923373 10608.62926631] nan\n",
      "---grad: [-1200.   -4400.    -350.59 -2800.  ]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "63 [-7327.83853685 -4042.28970636  3439.61460817 15808.63950904] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "64 [-8927.83853685 -8842.28970636  3357.12284485 13408.63950904] nan\n",
      "---grad: [-1200.   -4400.    -350.59 -2800.  ]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "65 [-7727.83853685 -4442.28970636  3707.71283895 16208.63950904] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "66 [-9327.83853683 -9242.28970635  3625.22107562 13808.63950906] nan\n",
      "---grad: [-1599.89 -5199.79  -371.21 -3599.79]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "67 [-7727.94489601 -4042.5024247   3996.42852696 17408.42679071] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "68 [-9327.94489601 -8842.5024247   3913.93676364 15008.42679071] nan\n",
      "---grad: [ -800.  -3600.   -536.2 -2000. ]\n",
      "---prob [0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "69 [-8527.94489601 -5242.5024247   4450.13322521 17008.42679071] nan\n",
      "---grad: [400.     0.    61.87 400.  ]\n",
      "---prob [0. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "95 [-11727.94489601  -4442.5024247    1026.72504748  17008.42679071] nan\n",
      "---grad: [400.     0.    61.87 400.  ]\n",
      "---prob [0. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "96 [-12127.94489601  -4442.5024247     964.856225    16608.42679071] nan\n",
      "---grad: [   0.   -400.    288.72    0.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "97 [-12127.94489601  -4042.5024247     676.13505338  16608.42679071] nan\n",
      "---grad: [400.     0.    61.87 400.  ]\n",
      "---prob [0. 1. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "98 [-12527.94489601  -4042.5024247     614.26623089  16208.42679071] nan\n",
      "---grad: [   0.   -400.    288.72    0.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "99 [-12527.94489601  -3642.5024247     325.54505928  16208.42679071] nan\n",
      "---grad: [ 400.   3600.   1134.26 1200.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 1. 0. 1.]\n",
      "100 [-12927.94489601  -7242.5024247    -808.71668635  15008.42679071] nan\n",
      "---grad: [ -800.  -3600.   -536.2 -2000. ]\n",
      "---prob [0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "101 [-12127.94489601  -3642.5024247    -272.52022478  17008.42679071] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "126 [-16527.94489609  -8442.50242478  -1448.027852    16208.42679063] nan\n",
      "---grad: [-1600.   -5200.    -371.21 -3600.  ]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "127 [-14927.94489609  -3242.50242478  -1076.81491706  19808.42679063] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "128 [-16527.94489609  -8042.50242478  -1159.30668038  17408.42679063] nan\n",
      "---grad: [ -800.  -3600.   -536.2 -2000. ]\n",
      "---prob [0. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "129 [-15727.94489609  -4442.50242478   -623.11021881  19408.42679063] nan\n",
      "---grad: [   0.   -400.    288.72    0.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "130 [-15727.94489609  -4042.50242478   -911.83139042  19408.42679063] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "131 [-17327.94489609  -8842.50242478   -994.32315374  17008.42679063] nan\n",
      "---grad: [-1600.   -5200.    -371.21 -3600.  ]\n",
      "---prob [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "132 [-15727.94489609  -3642.50242478   -623.11021881  20608.42679063] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "158 [-19727.94489676  -8042.50242478  -2767.89606485  18608.42678996] nan\n",
      "---grad: [-1200.   -3600.    -391.84 -2400.  ]\n",
      "---prob [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "159 [-18527.94489676  -4442.50242478  -2376.06018909  21008.42678996] nan\n",
      "---grad: [ 800.   400.  -536.2  800. ]\n",
      "---prob [1. 0. 1. 0. 1. 1. 1. 0. 1. 1.]\n",
      "160 [-19327.94489676  -4842.50242478  -1839.86372752  20208.42678996] nan\n",
      "---grad: [   0.   -400.    288.72    0.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "161 [-19327.94489676  -4442.50242478  -2128.58489913  20208.42678996] nan\n",
      "---grad: [   0.   -400.    288.72    0.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "162 [-19327.94489676  -4042.50242478  -2417.30607075  20208.42678996] nan\n",
      "---grad: [   0.   -400.    288.72    0.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "163 [-19327.94489676  -3642.50242478  -2706.02724236  20208.42678996] nan\n",
      "---grad: [ 400.      0.   -144.36  400.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 1. 1.]\n",
      "164 [-19727.94489676  -3642.50242478  -2561.66665656  19808.42678996] nan\n",
      "---grad: [   0.   -400.    288.72    0.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "189 [-22537.97306839  -7662.55876797  -4232.64188966  18988.37044677] nan\n",
      "---grad: [-1600.   -4400.    -412.46 -3200.  ]\n",
      "---prob [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "190 [-20937.97306839  -3262.55876797  -3820.18307306  22188.37044677] nan\n",
      "---grad: [1600.   4800.     82.49 2400.  ]\n",
      "---prob [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "191 [-22537.97306839  -8062.55876797  -3902.67483638  19788.37044677] nan\n",
      "---grad: [-1200.   -3600.    -391.84 -2400.  ]\n",
      "---prob [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "192 [-21337.97306839  -4462.55876797  -3510.83896062  22188.37044677] nan\n",
      "---grad: [ 400.      0.   -144.36  400.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 1. 1.]\n",
      "193 [-21737.97306839  -4462.55876797  -3366.47837481  21788.37044677] nan\n",
      "---grad: [   0.   -400.    288.72    0.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "194 [-21737.97306839  -4062.55876797  -3655.19954643  21788.37044677] nan\n",
      "---grad: [   0.   -400.    288.72    0.  ]\n",
      "---prob [0. 0. 1. 0. 1. 1. 1. 0. 0. 1.]\n",
      "195 [-21737.97306839  -3662.55876797  -3943.92071804  21788.37044677] nan\n",
      "---grad: [ 800.   400.  -536.2  800. ]\n",
      "---prob [1. 0. 1. 0. 1. 1. 1. 0. 1. 1.]\n",
      "221 [-24178.19059285  -4064.73401256  -4565.13105141  21267.71787339] nan\n",
      "---grad: [-400.   -400.    433.08 -400.  ]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 0. 1.]\n",
      "222 [-23778.19059285  -3664.73401256  -4998.21280883  21667.71787339] nan\n",
      "---grad: [-400.   -400.    433.08 -400.  ]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 0. 1.]\n",
      "223 [-23378.19059285  -3264.73401256  -5431.29456625  22067.71787339] nan\n",
      "---grad: [ 800.   400.  -536.2  800. ]\n",
      "---prob [1. 0. 1. 0. 1. 1. 1. 0. 1. 1.]\n",
      "224 [-24178.19059285  -3664.73401256  -4895.09810468  21267.71787339] nan\n",
      "---grad: [-400.   -400.    433.08 -400.  ]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 0. 1.]\n",
      "225 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "226 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "227 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "252 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "253 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "254 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "255 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "256 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "257 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "258 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "284 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "285 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "286 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "287 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "288 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "289 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "290 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "315 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
      "316 [-23778.19059285  -3264.73401256  -5328.1798621   21667.71787339] nan\n",
      "---grad: [ 0.  0. -0.  0.]\n",
      "---prob [0. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# в eval_model  использую мой calc_logloss1, чтобы избежать ошибок с y_pred~~0:\n",
    "W = eval_model(X_st, y, iterations=315, alpha=4000) \n",
    "\n",
    "# получился какой-то очень хаотичный спуск, но супер быстро.  \n",
    "# Градиент поначалу скачет, потом резко становится близким к 0 вместе с logloss и там стабилизируется.см. ниже после 221 шага!\n",
    "# Не знаю, насколько это корректно. Похоже не на итерации, на рандомный поиск )). \n",
    "# Не уверен, что процедура сходится статистически значимо. \n",
    "# Вроде у Энга было, что логлосс не является глобально выпуклой и имеет серию локальных минимумов, возможно \n",
    "# эти скачки позволяют в глобальный минимум запрыгнуть...\n",
    "\n",
    "# Скорость зависит от рандомной инициализации:\n",
    "# random.seed = 45: 256 [-23823.28578812  -3085.67164194  -5956.43811516  20851.52257769] 9.000000090222834e-09\n",
    "# random.seed = 47: 256 [-23909.77399145  -3231.55429043  -5388.57478411  21812.77384799] 8.000000080198074e-09\n",
    "# random.seed = 40: 511 [-24151.22320108  -3467.84817277  -5435.51514846  22175.95418217] 1.3409539231617882e-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Создайте функцию calc_pred_proba, возвращающую предсказанную вероятность класса 1 (на вход подаются W, который уже посчитан функцией eval_model и X, на выходе - массив y_pred_proba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pred_proba(X, W):\n",
    "    return sigmoid(W @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.22356227e-68, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 3.62253343e-82, 1.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 1.00000000e+00])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pred_proba(X_st, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Создайте функцию calc_pred, возвращающую предсказанный класс (на вход подаются W, который уже посчитан функцией eval_model и X, на выходе - массив y_pred)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pred(X, W):\n",
    "    return (sigmoid(W @ X) > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pred(X_st, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Посчитайте Accuracy, матрицу ошибок, точность и полноту, а также F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metrics():\n",
    "    TP = ( (calc_pred(X_st, W) == 1) & (y == 1)).astype('int').sum()\n",
    "    FP = ( (calc_pred(X_st, W) == 1) & (y == 0)).astype('int').sum()\n",
    "    TN = ( (calc_pred(X_st, W) == 0) & (y == 0)).astype('int').sum()\n",
    "    FN = ( (calc_pred(X_st, W) == 0) & (y == 1)).astype('int').sum() \n",
    "    \n",
    "    accuracy = ((calc_pred(X_st, W) == y).astype('int')).mean()\n",
    "    confus_matr = [[TP, FP],\n",
    "                  [TN, FN]]\n",
    "    \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    \n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    print(f'accuracy  {accuracy}')\n",
    "    print(f'confusion matrix {confus_matr}')\n",
    "    print(f'precision {precision}')\n",
    "    print(f'recall {recall}')\n",
    "    print(f'F1 {F1}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  1.0\n",
      "confusion matrix [[5, 0], [5, 0]]\n",
      "precision 1.0\n",
      "recall 1.0\n",
      "F1 1.0\n"
     ]
    }
   ],
   "source": [
    "my_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Могла ли модель переобучиться? Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "очевидно переобучилась, очень большие коэффициенты W. Нужна регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7*. Создайте функции eval_model_l1 и eval_model_l2 с применением L1 и L2 регуляризаций соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_logloss_l1(y, y_pred, w, lambd):\n",
    "    \n",
    "    y_pred = np.where(y_pred *(1 - y_pred) == 0, np.abs(y_pred - 10e-9), y_pred)\n",
    "    \n",
    "    err = - np.mean(y * np.log(y_pred) + (1.0 - y) * np.log(1.0 - y_pred)) + lambd * sum(np.abs(w))\n",
    "    err = np.sum(err)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_logloss_l2(y, y_pred, w, lambd):\n",
    "    \n",
    "    y_pred = np.where(y_pred *(1 - y_pred) == 0, np.abs(y_pred - 10e-9), y_pred)\n",
    "    \n",
    "    err = - np.mean(y * np.log(y_pred) + (1.0 - y) * np.log(1.0 - y_pred)) + lambd * sum(w * w)\n",
    "    err = np.sum(err)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_l1(X, y, iterations, alpha=1e-4, lambd=1):\n",
    "    np.random.seed(70)\n",
    "    W = np.random.randn(X.shape[0])\n",
    "    n = X.shape[1]\n",
    "    for i in range(1, iterations+2):\n",
    "        z = np.dot(W, X)\n",
    "        \n",
    "        y_pred = sigmoid(z)\n",
    "        err = calc_logloss_l1(y, y_pred, W, lambd)\n",
    "        \n",
    "        grad = alpha * (1/n * np.dot((y_pred - y), X.T) + lambd * (np.sign(W)))\n",
    "        \n",
    "        if i % (iterations / 10) == 1:\n",
    "            print(i, W, err)\n",
    "            #print('---grad:', grad.round(2))\n",
    "            #print('---prob', y_pred.round(2))\n",
    "        \n",
    "        W -= grad\n",
    "            \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 0.85099741  0.17775492 -0.85303971 -0.44389968] 1.0162974731357999\n",
      "91 [-1.22532394 -0.75968891  0.6419791   2.51717604] 0.5746462565457006\n",
      "181 [-2.18759917 -0.72399987  0.17341486  2.93758044] 0.5402961277435887\n",
      "271 [-2.68390078 -0.73968073  0.03038346  3.19703889] 0.5272579101100094\n",
      "361 [-2.82452387 -0.75755146  0.03411397  3.27170161] 0.522876239398357\n",
      "451 [-2.85711548 -0.76328689  0.05468975  3.29818103] 0.5238990813501256\n",
      "541 [-2.87625483 -0.762863    0.03583094  3.29423053] 0.5212441088757023\n",
      "631 [-2.88048377 -0.76295739  0.03497938  3.29619706] 0.5211684441083613\n",
      "721 [-2.87299586 -0.76772689  0.0546856   3.30623894] 0.5227278808423257\n",
      "811 [-2.87402524 -0.76373652  0.04888053  3.30690151] 0.5235301077291629\n",
      "901 [-2.87838753 -0.75872664  0.04341208  3.30137879] 0.5236379193154511\n"
     ]
    }
   ],
   "source": [
    "W = eval_model_l1(X_st, y, iterations=900, alpha=0.9, lambd=0.02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31833984, 0.31734158, 0.80134005, 0.00783083, 0.58881005,\n",
       "       0.56247427, 0.81760836, 0.02636697, 0.31858967, 0.80042226])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pred_proba(X_st, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pred(X_st, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  0.8\n",
      "confusion matrix [[4, 1], [4, 1]]\n",
      "precision 0.8\n",
      "recall 0.8\n",
      "F1 0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "my_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_l2(X, y, iterations, alpha=1e-4, lambd=1):\n",
    "    np.random.seed(70)\n",
    "    W = np.random.randn(X.shape[0])\n",
    "    n = X.shape[1]\n",
    "    for i in range(1, iterations+2):\n",
    "        z = np.dot(W, X)\n",
    "        \n",
    "        y_pred = sigmoid(z)\n",
    "        err = calc_logloss_l2(y, y_pred, W, lambd)\n",
    "        \n",
    "        grad = alpha * (1/n * np.dot((y_pred - y), X.T) + lambd * (W))\n",
    "        \n",
    "        if i % (iterations / 10) == 1:\n",
    "            print(i, W, err)\n",
    "            #print('---grad:', grad.round(2))\n",
    "            #print('---prob', y_pred.round(2))\n",
    "        \n",
    "        W -= grad\n",
    "            \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 0.85099741  0.17775492 -0.85303971 -0.44389968] 0.986588809446992\n",
      "91 [-1.32085824 -0.8141128   0.74507677  2.55774959] 0.5429644904720489\n",
      "181 [-1.89362793 -0.78337874  0.4574037   2.82309776] 0.5473921363391134\n",
      "271 [-2.05748017 -0.77519572  0.37841918  2.89463535] 0.549936124386587\n",
      "361 [-2.10365561 -0.77315641  0.35630242  2.91502833] 0.5507964810802898\n",
      "451 [-2.11667876 -0.77258862  0.35007502  2.92079482] 0.5510546779979679\n",
      "541 [-2.12035296 -0.77242912  0.34831907  2.92242268] 0.5511286894793126\n",
      "631 [-2.12138962 -0.77238417  0.34782371  2.92288206] 0.5511496644028082\n",
      "721 [-2.12168211 -0.77237149  0.34768395  2.92301167] 0.5511555898827053\n",
      "811 [-2.12176464 -0.77236792  0.34764452  2.92304825] 0.5511572623553216\n",
      "901 [-2.12178792 -0.77236691  0.34763339  2.92305857] 0.5511577342936376\n"
     ]
    }
   ],
   "source": [
    "W = eval_model_l2(X_st, y, iterations=900, alpha=0.9, lambd=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_pred(X_st, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  0.8\n",
      "confusion matrix [[4, 1], [4, 1]]\n",
      "precision 0.8\n",
      "recall 0.8\n",
      "F1 0.8000000000000002\n"
     ]
    }
   ],
   "source": [
    "my_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
